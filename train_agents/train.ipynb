{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from gymnasium.envs.registration import register\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import sys \n",
    "sys.path.append('/Applications/Studium/Master/Masterarbeit/AADRL/')\n",
    "from train_agents.eval_callback import EvaluationCallback\n",
    "from sb3_contrib import RecurrentPPO\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from util_functions import sample_recurrent_ppo_params,evaluate_policy\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "from torch import nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants & Register the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR                  = \"./training_eval\" # Where to store the results from training\n",
    "NUMBER_ENVS              = 4                 # How many environments at the same time\n",
    "N_STEP_VAL               = 2_500             # Total number of time steps after one wants to evaluate the policy \n",
    "N_EPISODE_VAL            = 3                 # Number of episodes to run every N_STEP_VAL steps\n",
    "TIMESTEPS_LEARNING       = 5e5               # How many timesteps the agent should perform\n",
    "SEED                     = 42                # To make the results reproducible\n",
    "PATH_ENV_DATA            = '/Applications/Studium/Master/Masterarbeit/AADRL/data/agent_environment_data.csv' # Path where the final environment data is stored\n",
    "PATH_TURBULENCE_INDX     = '/Applications/Studium/Master/Masterarbeit/AADRL/data/turbulence_index.csv'       # Where the file containing the turbulence index is stored\n",
    "STARTING_DATE_VALIDATION = '2014-10-01'      # Date from which the validation dataset should start\n",
    "STARTING_DATE_TEST       = '2016-01-04'      # Date from which the test dataset should start\n",
    "NUM_TRAILS_PARAMTUNING   = 50                # How many trials one wants to run for the hyperparameter estimation\n",
    "TIMESTEPS_TRAIN_TUNING   = 1e5               # Number of timesteps during each trial of hyperparameter estimation (lower than TIMESTEPS_LEARNING due to restricted computational resources) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment as a gym environment\n",
    "register(\n",
    "    id=\"Trading-v4\", \n",
    "    entry_point=\"env.multistock_trading_v4:MultiStockTrading\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Train, Validation and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the complete data containing the market data, technical indicators and sentiments\n",
    "agent_environment_data = pd.read_csv(PATH_ENV_DATA, index_col = 0, header = [0,1])\n",
    "\n",
    "# Read the turbulence index datataframe\n",
    "turbulence_index = pd.read_csv(PATH_TURBULENCE_INDX, index_col=0)\n",
    "turbulence_index = turbulence_index.fillna(0) # first turbulence index value is nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the datasets\n",
    "training_data = agent_environment_data.loc[:agent_environment_data.index[agent_environment_data.index == STARTING_DATE_VALIDATION][0]].iloc[:-1]\n",
    "validation_data = agent_environment_data.loc[agent_environment_data.index[agent_environment_data.index == STARTING_DATE_VALIDATION][0]:\n",
    "                                             agent_environment_data.index[agent_environment_data.index == STARTING_DATE_TEST][0]].iloc[:-1]\n",
    "testing_data = agent_environment_data.loc[agent_environment_data.index[agent_environment_data.index == STARTING_DATE_TEST][0]:]\n",
    "\n",
    "# Split the turbulence index also into training, validation and testing data\n",
    "turbulence_training = turbulence_index.iloc[:len(training_data)]\n",
    "turbulence_validation = turbulence_index.iloc[len(training_data):len(training_data)+len(validation_data)]\n",
    "turbulence_testing = turbulence_index.iloc[len(training_data)+len(validation_data):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agents\n",
    "\n",
    "Four agents will be trained in total. The configuration of each agent will be as follows:\n",
    "\n",
    "| Agent    | Network Architecture | Sentiments Used | Reward Function                           |\n",
    "|----------|---------------------:|:---------------:|:-----------------------------------------:|\n",
    "| 1        |        MLP           |       ✗         |  Absolute Portfolio Change                |\n",
    "| 2        |        MLP           |       ✓         |  Absolute Portfolio Change                |\n",
    "| 3        |        MLP           |       ✓         |  Portfolio Return  & Sharpe Ratio         |\n",
    "| 4        |        MLP + LSTM    |       ✓         |  Portfolio Return  & Sharpe Ratio         |\n",
    "\n",
    "Note that the 4th agent represents the final agent containin all model changes. To find the hyperparameters for which the agent performs the best, a hyperparamter optimization using Optuna will be done. This will only be done for the fourth agent. The other agents will then get the same hyperparameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter estimation (using Optuna) regarding the setup of the 4th agent\n",
    "Note that because the training takes so long, only 100k timesteps will be performed during each training of the hyperparameter optimization (instead of the full 500k that will be used later). Afterwards, the best agent will be trained with 500k steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    \"\"\" \n",
    "    Run the hyperparameter estimation using Optuna \n",
    "\n",
    "    Please note that this method is based on the comment:\n",
    "    https://github.com/araffin/rl-baselines-zoo/issues/29#issuecomment-646888620\n",
    "    \"\"\"\n",
    "    # Sample the algorithm specific hyperparameters\n",
    "    model_params = sample_recurrent_ppo_params(trial)\n",
    "    \n",
    "    # Sample the window for the sharpe ratio (number of actions, note that after performing w actions, \n",
    "    # there are w+1 portfolio values to calculate the the sharpe ratio)\n",
    "    n_step_sharpe = trial.suggest_categorical(\"n_step_sharpe\", [99,199,299])\n",
    "\n",
    "    # Define the Training environment\n",
    "    env = make_vec_env('Trading-v4', \n",
    "                        n_envs=4,\n",
    "                        env_kwargs = {\"market_data\":training_data, \n",
    "                                      \"turbulence_index\":turbulence_training,\n",
    "                                      \"consider_sentiments\":True, \n",
    "                                      \"n_step_sharpe\":n_step_sharpe},\n",
    "                        seed=SEED)\n",
    "\n",
    "    # Define the environment to evaluate the agent on (validation dataset will be used)\n",
    "    eval_env = gym.make(\n",
    "                        'Trading-v4', \n",
    "                        market_data=validation_data, \n",
    "                        turbulence_index=turbulence_validation,\n",
    "                        consider_sentiments=True, \n",
    "                        n_step_sharpe=n_step_sharpe,\n",
    "                        seed=SEED\n",
    "                        )\n",
    "\n",
    "    # Define the recurrent ppo model with the sampled hyperparameters\n",
    "    model = RecurrentPPO(\n",
    "                    'MlpLstmPolicy', \n",
    "                    env, \n",
    "                    verbose=0, \n",
    "                    seed=SEED,\n",
    "                    **model_params\n",
    "                    )\n",
    "    \n",
    "    # Define the evaluation callback to evaluate the policy \n",
    "    eval_callback_params_tuning = EvaluationCallback(\n",
    "                                        eval_env,\n",
    "                                        eval_freq=N_STEP_VAL*NUMBER_ENVS,\n",
    "                                        n_eval_episodes=N_EPISODE_VAL, \n",
    "                                        save_every_nstep=None, # Don't save intermediate agents to save memory\n",
    "                                        log_path=LOG_DIR + '/fourth_agent_hpt_2',\n",
    "                                        verbose=0\n",
    "                                        )\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(TIMESTEPS_TRAIN_TUNING,callback=eval_callback_params_tuning)    \n",
    "\n",
    "    # Load the model that achieved the highest rewards on the validation dataset\n",
    "    model = RecurrentPPO.load(\n",
    "        \"/Applications/Studium/Master/Masterarbeit/AADRL/train_agents/training_eval/fourth_agent_hpt_2/best_model.zip\", \n",
    "        env=eval_env\n",
    "        )\n",
    "    \n",
    "    # Run the policy 5 times over the validation dataset and calculate the mean episodic return\n",
    "    _, _, _, mean_reward, _ = evaluate_policy(eval_env, model,n_eval_episodes=5,deterministic=False)\n",
    "\n",
    "    return -mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-02 16:17:20,233] A new study created in memory with name: no-name-640b807a-164d-471c-a967-8cb7daf96258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9477607ab42a4c1280e5e620717e3d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-09-02 16:40:46,307] Trial 0 finished with value: -3.1962892368447253 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 99}. Best is trial 0 with value: -3.1962892368447253.\n",
      "[I 2024-09-02 16:54:04,063] Trial 1 finished with value: -3.745568366083293 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0001, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 1 with value: -3.745568366083293.\n",
      "[I 2024-09-02 17:48:34,220] Trial 2 finished with value: 1.598771836458777 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 99}. Best is trial 1 with value: -3.745568366083293.\n",
      "[I 2024-09-02 18:56:20,805] Trial 3 finished with value: -10.683811373007774 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 199}. Best is trial 3 with value: -10.683811373007774.\n",
      "[I 2024-09-02 19:07:02,756] Trial 4 finished with value: -3.393719792447597 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 3 with value: -10.683811373007774.\n",
      "[I 2024-09-02 19:19:06,066] Trial 5 finished with value: -8.224112835384364 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 3 with value: -10.683811373007774.\n",
      "[I 2024-09-02 19:51:20,920] Trial 6 finished with value: -8.811730828238254 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 0.4, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 99}. Best is trial 3 with value: -10.683811373007774.\n",
      "[I 2024-09-02 20:00:38,014] Trial 7 finished with value: -33.51940969628868 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 99}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 20:09:51,916] Trial 8 finished with value: -12.24975518504824 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 99}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 20:41:38,260] Trial 9 finished with value: -0.6212370174736901 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 299}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 21:15:30,503] Trial 10 finished with value: -12.433575218117225 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 99}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 21:36:48,816] Trial 11 finished with value: -2.0608708610389166 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 99}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 21:49:24,880] Trial 12 finished with value: -11.764238745339233 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 22:38:17,258] Trial 13 finished with value: 0.7855449085247086 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 199}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 22:48:06,804] Trial 14 finished with value: -2.5502077810271233 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 23:04:43,645] Trial 15 finished with value: -0.25989061785962114 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 199}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 23:27:41,431] Trial 16 finished with value: -3.628699064269027 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 23:42:21,322] Trial 17 finished with value: -12.571293601608442 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.4, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 23:53:32,816] Trial 18 finished with value: -1.986320625775776 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.95, 'max_grad_norm': 0.4, 'net_arch': 'small', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-02 23:58:52,010] Trial 19 finished with value: -0.6392740846794366 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 199}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-03 00:15:48,347] Trial 20 finished with value: -7.094351003822474 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0005, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 99}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-03 00:36:16,424] Trial 21 finished with value: -5.888809740753137 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0001, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 7 with value: -33.51940969628868.\n",
      "[I 2024-09-03 00:43:55,212] Trial 22 finished with value: -33.57347287968251 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0005, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 00:55:55,849] Trial 23 finished with value: -10.260265351643412 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 01:09:04,733] Trial 24 finished with value: -9.632218889167714 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 01:16:45,807] Trial 25 finished with value: -4.422470100688669 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 01:29:54,387] Trial 26 finished with value: -5.823903488606213 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 01:35:11,213] Trial 27 finished with value: -7.589842068399098 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 01:44:18,634] Trial 28 finished with value: -13.113933954555364 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 02:32:32,167] Trial 29 finished with value: -8.717231928612852 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 02:43:06,841] Trial 30 finished with value: -3.7522083891599833 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.95, 'max_grad_norm': 0.4, 'net_arch': 'small', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 03:05:25,741] Trial 31 finished with value: -12.882898993602478 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 03:49:28,692] Trial 32 finished with value: 0.05231951880260351 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 04:08:07,559] Trial 33 finished with value: -9.815427717966829 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 04:24:16,768] Trial 34 finished with value: -7.9899829561121205 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.95, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 04:29:34,185] Trial 35 finished with value: -1.9124910223781968 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 04:44:01,210] Trial 36 finished with value: -3.632520722557652 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 05:11:16,426] Trial 37 finished with value: 4.500378185281453 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.4, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 05:54:41,292] Trial 38 finished with value: -0.9426474960214584 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 06:03:30,236] Trial 39 finished with value: -7.396638454194962 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 06:08:16,466] Trial 40 finished with value: -7.798387547074715 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 06:22:42,592] Trial 41 finished with value: -12.511557523833451 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 07:01:58,861] Trial 42 finished with value: -2.226541286126108 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 256, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 07:54:18,289] Trial 43 finished with value: -8.73119615420811 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.1, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 07:59:19,062] Trial 44 finished with value: -2.2535798715553472 and parameters: {'batch_size': 64, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.4, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 08:04:50,374] Trial 45 finished with value: -11.617409907254316 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0005, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.95, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 64, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 08:29:16,637] Trial 46 finished with value: -15.91210662511792 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 0.0001, 'clip_range': 0.2, 'n_epochs': 15, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 99}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 08:47:50,414] Trial 47 finished with value: -2.984981940314354 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.99, 'learning_rate': 0.0005, 'clip_range': 0.1, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 256, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 09:02:12,496] Trial 48 finished with value: -5.435871956164105 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.99, 'learning_rate': 0.0001, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'net_arch': 'small', 'lstm_hidden_size': 128, 'n_step_sharpe': 199}. Best is trial 22 with value: -33.57347287968251.\n",
      "[I 2024-09-03 09:17:06,798] Trial 49 finished with value: -4.0521205846122355 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0003, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.3, 'net_arch': 'medium', 'lstm_hidden_size': 128, 'n_step_sharpe': 299}. Best is trial 22 with value: -33.57347287968251.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(sampler=RandomSampler(seed=SEED))\n",
    "study.optimize(optimize_agent,n_trials=NUM_TRAILS_PARAMTUNING,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'n_steps': 128,\n",
       " 'gamma': 0.999,\n",
       " 'learning_rate': 0.0005,\n",
       " 'clip_range': 0.2,\n",
       " 'n_epochs': 5,\n",
       " 'gae_lambda': 0.98,\n",
       " 'max_grad_norm': 0.3,\n",
       " 'net_arch': 'small',\n",
       " 'lstm_hidden_size': 128,\n",
       " 'n_step_sharpe': 299}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary containing the best parameters from the hyperparameter estimation\n",
    "best_params = {\n",
    "    'batch_size': study.best_params['batch_size'],\n",
    "    'n_steps': study.best_params['n_steps'],\n",
    "    'gamma': study.best_params['gamma'],\n",
    "    'learning_rate': study.best_params['learning_rate'],\n",
    "    'clip_range': study.best_params['clip_range'],\n",
    "    'n_epochs': study.best_params['n_epochs'],\n",
    "    'gae_lambda': study.best_params['gae_lambda'],\n",
    "    'max_grad_norm': study.best_params['max_grad_norm'],\n",
    "    'policy_kwargs': {\n",
    "        'net_arch': dict(pi=[64, 64], vf=[64, 64]) if study.best_params['net_arch'] == 'small' else dict(pi=[256, 64], vf=[256, 64]),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Additional parameters for RecurrentPPO\n",
    "recurrent_params = copy.deepcopy(best_params) \n",
    "recurrent_params['policy_kwargs'].update({\n",
    "                                        'lstm_hidden_size': study.best_params['lstm_hidden_size'],\n",
    "                                    })\n",
    "\n",
    "N_STEP_SHARPE=study.best_params['n_step_sharpe']\n",
    "\n",
    "# Create Pandas Dataframe and save the results\n",
    "best_params_csv = pd.DataFrame.from_dict(study.best_params,orient='index').T\n",
    "best_params_csv.to_csv(r'/Applications/Studium/Master/Masterarbeit/AADRL/train_agents/training_eval/best_params_hyperparameteropt_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agents \n",
    "\n",
    "Multiple SEEDs will be used to have a better overview on the agents performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combinations of whether to consider sentiments and sharpe ratio into reward function \n",
    "consider_sentiments = [False, True, True, True]\n",
    "consider_sharpes = [None, None, N_STEP_SHARPE, N_STEP_SHARPE] # When None, the sharpe ratio will not be considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [42, 7, 25, 14]  # Random seeds used for training (chosen arbitrarily but must be fixed to ensure reproducible results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the random seeds and execute the training\n",
    "for seed in random_seeds:\n",
    "    # Loop over the different configurations and train the agent\n",
    "    for agent_num, (consider_sentiment, consider_sharpe) in enumerate(zip(consider_sentiments, consider_sharpes)):\n",
    "\n",
    "        # Define the training environment\n",
    "        env = make_vec_env('Trading-v4', \n",
    "                            n_envs=4,\n",
    "                            env_kwargs = {\"market_data\":training_data, \n",
    "                                        \"turbulence_index\":turbulence_training,\n",
    "                                        \"consider_sentiments\":consider_sentiment, \n",
    "                                        \"n_step_sharpe\":consider_sharpe}\n",
    "                            ) # Seed does not need to be specified (has no effect) -> needs to be specified in model definition\n",
    "\n",
    "        # Define the environment to evaluate the agent on the validation dataset\n",
    "        eval_env_val = gym.make(\n",
    "                            'Trading-v4', \n",
    "                            market_data=validation_data, \n",
    "                            turbulence_index=turbulence_validation,\n",
    "                            consider_sentiments=consider_sentiment, \n",
    "                            n_step_sharpe=consider_sharpe,\n",
    "                            )\n",
    "        \n",
    "        # Define the agent: Use RecurrentPPO if it is the fourth agent, otherwise PPO\n",
    "        if agent_num != 3:\n",
    "            model_agent = PPO(\n",
    "                'MlpPolicy',\n",
    "                env,\n",
    "                seed=seed,\n",
    "                verbose=0,\n",
    "                **best_params\n",
    "            )\n",
    "        else:\n",
    "            model_agent = RecurrentPPO(\n",
    "                'MlpLstmPolicy',\n",
    "                env,\n",
    "                seed=seed,\n",
    "                verbose=0,\n",
    "                **recurrent_params\n",
    "            )\n",
    "\n",
    "        # Define the evaluation callback (to save the best performing agent and saving the agent every n-th step)\n",
    "        eval_callback_validation = EvaluationCallback(\n",
    "                                            eval_env_val,\n",
    "                                            eval_freq=N_STEP_VAL*NUMBER_ENVS,\n",
    "                                            n_eval_episodes=N_EPISODE_VAL, \n",
    "                                            save_every_nstep=True, \n",
    "                                            saving_freq=10000,\n",
    "                                            log_path=LOG_DIR + f'/agent_{agent_num + 1}_seed_{seed}',\n",
    "                                            verbose=0,\n",
    "                                            additional_name_eval_file='validation'\n",
    "                                        )\n",
    "\n",
    "        # Train the agent\n",
    "        model_agent.learn(\n",
    "            total_timesteps=5e5,\n",
    "            callback=eval_callback_validation,\n",
    "            progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data\n",
    "training_data.to_csv('/Applications/Studium/Master/Masterarbeit/AADRL/data/training/market_data.csv')\n",
    "turbulence_training.to_csv('/Applications/Studium/Master/Masterarbeit/AADRL/data/training/turbulence_index.csv')\n",
    "\n",
    "# Save the validation data\n",
    "validation_data.to_csv('/Applications/Studium/Master/Masterarbeit/AADRL/data/validation/market_data.csv')\n",
    "turbulence_validation.to_csv('/Applications/Studium/Master/Masterarbeit/AADRL/data/validation/turbulence_index.csv')\n",
    "\n",
    "# Save the test data\n",
    "testing_data.to_csv('/Applications/Studium/Master/Masterarbeit/AADRL/data/testing/market_data.csv')\n",
    "turbulence_testing.to_csv('/Applications/Studium/Master/Masterarbeit/AADRL/data/testing/turbulence_index.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
